{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "import os\n",
    "import praw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read_file(open(\"../secrets/credentials.cfg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_id: str = config.get(\"reddit\", \"client_id\")\n",
    "client_secret: str = config.get(\"reddit\", \"client_secret\")\n",
    "user_agent: str = config.get(\"reddit\", \"user_agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = praw.Reddit(\n",
    "            user_agent = user_agent,\n",
    "            client_id = client_id,\n",
    "            client_secret = client_secret\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '14a4mh0', 'name': 't3_14a4mh0', 'author': 'oneohsevenam', 'title': 'Mac vs PC?', 'selftext': \"I've used Mac for years. I need to get a new laptop, though, and I'm *so* tempted to switch to PC (mostly I just like the UI better). But I know a lot of people say that Mac is faster/more worthwhile for data science. Any strong opinions? And/or recs for a good laptop that you use for work?\\n\\ncontext: I work from home on my own laptop\", 'subreddit': 'datascience', 'upvotes': 1, 'downvotes': 0.0, 'over_18': False, 'timestamp': 1686841206.0, 'url': 'https://www.reddit.com/r/datascience/comments/14a4mh0/mac_vs_pc/'}\n",
      "{'id': '14a4lf8', 'name': 't3_14a4lf8', 'author': 'Skoltech_', 'title': \"After AlphaFold cracked a 50-year-old problem of structural bioinformatics by learning to predict protein structures, it couldn't solve the mutant protein stability problem. And the AI has not really “learned” the physics of proteins either, a new study concludes.\", 'selftext': '', 'subreddit': 'datascience', 'upvotes': 1, 'downvotes': 0.0, 'over_18': False, 'timestamp': 1686841137.0, 'url': 'https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0282689'}\n",
      "{'id': '14a45iu', 'name': 't3_14a45iu', 'author': 'dmitryspodarets', 'title': 'Webinar \"Unlocking Data Value with Large Language Models\"', 'selftext': '', 'subreddit': 'datascience', 'upvotes': 1, 'downvotes': 0.0, 'over_18': False, 'timestamp': 1686840039.0, 'url': 'https://youtube.com/live/ztEYvHeIYow?feature=share'}\n",
      "{'id': '14a44ca', 'name': 't3_14a44ca', 'author': 'eaazzy-eeee', 'title': 'Predictive analysis as a web developer', 'selftext': \"Hi guys, I am full-stack web developer. I am going to build a web app which will make data predictions. For example, I sell three different chocolate brands at my shop which I order from a wholesaler. I want my app to make predictions for my next order according to the sales. But I don't have any knowledge of predictive analysis or data science. Though I know Python. I want to start learning Data science and I want it learn in way so that I can create this app. Can someone tell me how should I start and what should I learn?\", 'subreddit': 'datascience', 'upvotes': 1, 'downvotes': 0.0, 'over_18': False, 'timestamp': 1686839959.0, 'url': 'https://www.reddit.com/r/datascience/comments/14a44ca/predictive_analysis_as_a_web_developer/'}\n",
      "{'id': '14a41vs', 'name': 't3_14a41vs', 'author': 'PathSalty7764', 'title': 'Math as vector for self education', 'selftext': \"School vacations have already started, and I'm a little bored. In the future I would like to go into a profession related to mathematics. Some of its areas seem particularly interesting to me (eg statistics and mix of areas like data science). Does it make sense to take mathematics as a vector of self-development and study it according to the road map?\", 'subreddit': 'datascience', 'upvotes': 0, 'downvotes': 0.0, 'over_18': False, 'timestamp': 1686839788.0, 'url': 'https://i.redd.it/mmj37j4q076b1.jpg'}\n",
      "{'id': '14a41c4', 'name': 't3_14a41c4', 'author': 'ww3ace', 'title': \"[D] r/machinelearning caused the API rate hike that they're protesting\", 'selftext': \"We as data scientists and machine learning engineers have been designing really powerful language models and we've been using reddit datasets to model conversations for at least a decade. Finally these language models are starting to become useful and all sorts of [companies are looking to control or monetize the use of their data in training language models.](https://techmonitor.ai/technology/software/reddit-api-blackout-price-hike#) This is the inevitable result of designing powerful language models: data becomes more valuable. We killed 3rd party reddit apps.\", 'subreddit': 'datascience', 'upvotes': 0, 'downvotes': 0.0, 'over_18': False, 'timestamp': 1686839749.0, 'url': 'https://www.reddit.com/r/datascience/comments/14a41c4/d_rmachinelearning_caused_the_api_rate_hike_that/'}\n",
      "{'id': '14a3x3f', 'name': 't3_14a3x3f', 'author': 'mikehawk1988', 'title': 'Normalization', 'selftext': \"Another post of mine (non data scientist). This is rather a statistics problem.     \\n\\nI have about 120 samples with about 6 factors/features.  I really just want to examine the effect of 1 of those 6 factors. However, when I split my data according to the two factor levels that I'm trying to compute the effect of my two samples to compare are not normally distributed so that I cannot perform a t-test. If I rescale/normalize those factors by just computing a mean I essentially reduce my sample size but I get normally distributed samples.    \\n\\nNow, I have a very stupid comprehension problem.      \\n1. Should I average out my data to make it normally distributed? (imagine it like having a bimodal distribution that arises from two distributions. But if I pool these distributions into one by averaging and essentially reducing the sample size by a factor of 2 I get a normal distribution. I assume that both distributions just differ by an offset since one sample was taken on day 1 and the other on day 7).         \\n2. Should I not average out and instead use another test other than a t-test that doesn't require a normal distribution?        \\n\\nThanks!\", 'subreddit': 'datascience', 'upvotes': 1, 'downvotes': 0.0, 'over_18': False, 'timestamp': 1686839451.0, 'url': 'https://www.reddit.com/r/datascience/comments/14a3x3f/normalization/'}\n",
      "{'id': '14a3w4z', 'name': 't3_14a3w4z', 'author': 'f-n-f', 'title': 'Are you negotiating in this economy?', 'selftext': '\\n\\n[View Poll](https://www.reddit.com/poll/14a3w4z)', 'subreddit': 'datascience', 'upvotes': 1, 'downvotes': 0.0, 'over_18': False, 'timestamp': 1686839390.0, 'url': 'https://www.reddit.com/r/datascience/comments/14a3w4z/are_you_negotiating_in_this_economy/'}\n",
      "{'id': '14a388u', 'name': 't3_14a388u', 'author': 'Koalashart1', 'title': 'There’s a lot of data science books out there, any recommendations for must-reads?', 'selftext': '', 'subreddit': 'datascience', 'upvotes': 1, 'downvotes': 0.4925373134328357, 'over_18': False, 'timestamp': 1686837713.0, 'url': 'https://www.reddit.com/r/datascience/comments/14a388u/theres_a_lot_of_data_science_books_out_there_any/'}\n",
      "{'id': '14a32mb', 'name': 't3_14a32mb', 'author': 'RyhanSunny_Altinity', 'title': 'Fortress ClickHouse: Secure Your Database and Foil Evildoers in 15 Minutes or Less', 'selftext': 'Are you interested in securing your sensitive data on ClickHouse and making it hacker-proof? Robert Hodges from Altinity will walk you through exactly that with a LIVE demo today. The webinar starts in a few hours at 10 AM PDT today, June 15th. Please RSVP your virtual free seat to join this live educational webinar where we will share a cookbook for you to fully lock down your ClickHouse servers!\\n\\n[https://hubs.la/Q01NztWn0](https://hubs.la/Q01NztWn0)\\n\\nhttps://preview.redd.it/5uoyupsbt66b1.jpg?width=1024&format=pjpg&auto=webp&v=enabled&s=0eacd0f8003dcf53d33a65ae241820e9f9dea228', 'subreddit': 'datascience', 'upvotes': 1, 'downvotes': 0.0, 'over_18': False, 'timestamp': 1686837321.0, 'url': 'https://www.reddit.com/r/datascience/comments/14a32mb/fortress_clickhouse_secure_your_database_and_foil/'}\n"
     ]
    }
   ],
   "source": [
    "subreddit = client.subreddit(\"datascience\")\n",
    "submissions = []\n",
    "for submission in subreddit.new(limit=2000):\n",
    "    submission_json: dict[str, str] = {\n",
    "        \"id\": submission.id,\n",
    "        \"name\": submission.name,\n",
    "        \"author\": submission.author.name,\n",
    "        \"title\": submission.title,\n",
    "        \"selftext\": submission.selftext,\n",
    "        \"subreddit\": submission.subreddit.display_name,\n",
    "        \"upvotes\": submission.score,\n",
    "        \"downvotes\": (submission.score / submission.upvote_ratio) - submission.score,\n",
    "        \"over_18\": submission.over_18,\n",
    "        \"timestamp\": submission.created_utc,\n",
    "        \"url\": submission.url,\n",
    "    }\n",
    "    print(submission_json)\n",
    "# print()\n",
    "# for comment in subreddit.comments():\n",
    "#     comment_json: dict[str, str] = {\n",
    "#         \"id\": comment.id,\n",
    "#         \"name\": comment.name,\n",
    "#         \"author\": comment.author.name,\n",
    "#         \"body\": comment.body,\n",
    "#         \"subreddit\": comment.subreddit.display_name,\n",
    "#         \"submission\": comment.submission.id,\n",
    "#         \"upvotes\": comment.ups,\n",
    "#         \"downvotes\": comment.downs,\n",
    "#         \"over_18\": comment.over_18,\n",
    "#         \"timestamp\": comment.created_utc,\n",
    "#         \"permalink\": comment.permalink,\n",
    "#     }\n",
    "#     print(comment_json)\n",
    "    \n",
    "    # break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(submissions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
